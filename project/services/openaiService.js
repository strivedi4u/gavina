const OpenAI = require('openai');
const freeAiService = require('./freeAiService');
const vectorDatabaseService = require('./vectorDatabaseService');
const advancedMemoryService = require('./advancedMemoryService');
const multimodalProcessingService = require('./multimodalProcessingService');
const logger = require('./loggerService');
require('dotenv').config();

class AdvancedOpenAIService {
  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
    
    this.models = {
      chat: process.env.OPENAI_MODEL || 'gpt-4o-2024-08-06',
      vision: process.env.VISION_MODEL || 'gpt-4o',
      embedding: process.env.EMBEDDING_MODEL || 'text-embedding-3-large'
    };
    
    this.systemPrompt = this.buildSystemPrompt();
  }

  buildSystemPrompt() {
    return `You are an advanced AI assistant with the following capabilities:

ðŸ§  MEMORY & CONTEXT:
- You have persistent memory and can remember past conversations
- You learn from user preferences and adapt your responses
- You maintain context across sessions and conversations

ðŸ” KNOWLEDGE RETRIEVAL:
- You can search through uploaded documents, images, audio, and videos
- You access a comprehensive vector database of processed information
- You retrieve relevant context for every query

ðŸŒ MULTIMODAL PROCESSING:
- You can analyze images, extract text from documents (PDF, Word, Excel)
- You can transcribe audio and video content
- You can process web pages and URLs
- You understand and work with multiple file formats

ðŸ“Š DATA ANALYSIS:
- You can create visualizations of vector relationships
- You analyze data patterns and connections
- You provide insights based on comprehensive data analysis

ðŸ’¡ ADVANCED REASONING:
- You use retrieval-augmented generation (RAG) for accurate responses
- You combine multiple sources of information intelligently
- You provide evidence-based answers with source attribution

INSTRUCTIONS:
1. Always check your memory and context before responding
2. Retrieve relevant information from the knowledge base
3. Provide comprehensive, well-sourced answers
4. Learn from interactions to improve future responses
5. Be helpful, accurate, and cite your sources when possible

Remember: You're not just answering questions - you're providing intelligent, contextual, and personalized assistance based on the user's entire interaction history and knowledge base.`;
  }

  async askQuestion(question, userId = 'default', context = '', options = {}) {
    try {
      const {
        includeMemory = true,
        includeContext = true,
        maxContextLength = 8000,
        temperature = 0.3,
        maxTokens = 2000
      } = options;

      logger.info(`Processing question from user ${userId}: ${question.substring(0, 100)}...`);

      // 1. Get user's memory and conversation history
      let memoryContext = '';
      if (includeMemory) {
        const memory = await advancedMemoryService.getContextualMemory(userId, question, 5);
        memoryContext = memory.summary;
      }

      // 2. Perform semantic search on vector database
      let retrievedContext = '';
      if (includeContext) {
        const searchResults = await vectorDatabaseService.similaritySearch(question, 5);
        retrievedContext = searchResults
          .map(result => `[Source: ${result.metadata.type || 'unknown'}] ${result.text}`)
          .join('\n\n');
      }

      // 3. Build comprehensive context
      const fullContext = this.buildContext({
        userContext: context,
        memoryContext,
        retrievedContext,
        maxLength: maxContextLength
      });

      // 4. Create enhanced prompt
      const messages = [
        { role: 'system', content: this.systemPrompt },
        { role: 'user', content: this.buildPrompt(question, fullContext) }
      ];

      // 5. Get response from GPT
      let response;
      if (process.env.OPENAI_API_KEY && process.env.OPENAI_API_KEY.length > 20) {
        const completion = await this.openai.chat.completions.create({
          model: this.models.chat,
          messages,
          temperature,
          max_tokens: maxTokens,
          presence_penalty: 0.1,
          frequency_penalty: 0.1
        });

        response = completion.choices[0].message.content;
        
        // Add model information
        response = `**[Generated by ${this.models.chat}]**\n\n${response}`;
      } else {
        throw new Error('OpenAI API key not configured');
      }

      // 6. Store conversation in memory
      if (includeMemory) {
        await advancedMemoryService.storeConversation(userId, question, response, {
          model: this.models.chat,
          contextLength: fullContext.length,
          timestamp: new Date().toISOString()
        });

        // Extract and store important information
        await this.extractAndStoreMemories(userId, question, response);
      }

      // 7. Store interaction in vector database for future retrieval
      await vectorDatabaseService.createEmbedding(
        `Q: ${question}\nA: ${response}`,
        {
          userId,
          type: 'qa_interaction',
          model: this.models.chat,
          timestamp: new Date().toISOString()
        }
      );

      logger.info(`Successfully generated response for user ${userId}`);
      return response;

    } catch (err) {
      logger.error('OpenAI API error:', err);
      
      // Fallback to free AI service
      logger.info('ðŸ”„ Switching to free AI models...');
      return await freeAiService.generateResponse(context, question);
    }
  }

  async analyzeImage(imagePath, question = "What do you see in this image?", userId = 'default') {
    try {
      if (!process.env.ENABLE_VISION || process.env.ENABLE_VISION !== 'true') {
        throw new Error('Vision analysis is disabled');
      }

      logger.info(`Analyzing image for user ${userId}`);

      // Process the image first to extract text and metadata
      const processedImage = await multimodalProcessingService.processFile(
        imagePath, 
        'uploaded_image.jpg', 
        { userId, analysisType: 'vision' }
      );

      // Read the image file
      const fs = require('fs-extra');
      const imageBuffer = await fs.readFile(imagePath);
      const base64Image = imageBuffer.toString('base64');

      // Analyze with GPT-4 Vision
      const response = await this.openai.chat.completions.create({
        model: this.models.vision,
        messages: [
          {
            role: 'system',
            content: 'You are an expert image analyst. Provide detailed, accurate descriptions of images and answer questions about their content.'
          },
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: `${question}\n\nAdditional context from OCR: ${processedImage.extractedText || 'No text detected'}`
              },
              {
                type: 'image_url',
                image_url: {
                  url: `data:image/jpeg;base64,${base64Image}`,
                  detail: 'high'
                }
              }
            ]
          }
        ],
        max_tokens: 1000
      });

      const analysis = response.choices[0].message.content;

      // Store the analysis in memory and vector database
      await advancedMemoryService.storeMemory(userId, 'image_analysis', {
        question,
        analysis,
        ocrText: processedImage.extractedText,
        imageInfo: processedImage.analysis.imageInfo
      }, {
        category: 'vision_analysis',
        importance: 3
      });

      const fullResponse = `**[Vision Analysis by ${this.models.vision}]**\n\n${analysis}`;

      logger.info(`Successfully analyzed image for user ${userId}`);
      return {
        analysis: fullResponse,
        extractedText: processedImage.extractedText,
        metadata: processedImage.analysis
      };

    } catch (error) {
      logger.error('Failed to analyze image:', error);
      throw error;
    }
  }

  async processMultimodalQuery(query, files = [], urls = [], userId = 'default') {
    try {
      logger.info(`Processing multimodal query for user ${userId} with ${files.length} files and ${urls.length} URLs`);

      let processedContent = [];
      let extractedTexts = [];

      // Process uploaded files
      for (const file of files) {
        const processed = await multimodalProcessingService.processFile(
          file.path,
          file.originalname,
          { userId, queryContext: query }
        );
        processedContent.push(processed);
        if (processed.extractedText) {
          extractedTexts.push(`[${file.originalname}] ${processed.extractedText}`);
        }
      }

      // Process URLs
      for (const url of urls) {
        const processed = await multimodalProcessingService.processUrl(
          url,
          { userId, queryContext: query }
        );
        processedContent.push(processed);
        if (processed.extractedText) {
          extractedTexts.push(`[${url}] ${processed.extractedText}`);
        }
      }

      // Combine all extracted content
      const combinedContext = extractedTexts.join('\n\n');

      // Generate response using the combined context
      const response = await this.askQuestion(query, userId, combinedContext, {
        maxContextLength: 12000, // Increased for multimodal content
        temperature: 0.2 // Lower temperature for factual content
      });

      // Store processing results in memory
      await advancedMemoryService.storeMemory(userId, 'multimodal_query', {
        query,
        filesProcessed: files.map(f => f.originalname),
        urlsProcessed: urls,
        contentSummary: combinedContext.substring(0, 500) + '...'
      }, {
        category: 'multimodal',
        importance: 4
      });

      return {
        response,
        processedContent,
        extractedTexts: extractedTexts.length,
        totalContent: combinedContext.length
      };

    } catch (error) {
      logger.error('Failed to process multimodal query:', error);
      throw error;
    }
  }

  buildContext({ userContext, memoryContext, retrievedContext, maxLength }) {
    let context = '';
    
    if (memoryContext) {
      context += `MEMORY & PREFERENCES:\n${memoryContext}\n\n`;
    }
    
    if (retrievedContext) {
      context += `RELEVANT KNOWLEDGE:\n${retrievedContext}\n\n`;
    }
    
    if (userContext) {
      context += `CURRENT CONTEXT:\n${userContext}\n\n`;
    }

    // Truncate if too long
    if (context.length > maxLength) {
      context = context.substring(0, maxLength) + '...\n[Context truncated]';
    }

    return context;
  }

  buildPrompt(question, context) {
    if (!context.trim()) {
      return question;
    }

    return `CONTEXT INFORMATION:
${context}

QUESTION: ${question}

INSTRUCTIONS: Based on the context provided above, please provide a comprehensive and accurate answer. If you reference information from the context, please indicate the source. If the context doesn't contain relevant information, clearly state that and provide the best answer you can based on your training data.`;
  }

  async extractAndStoreMemories(userId, question, response) {
    try {
      // Use GPT to extract important information that should be remembered
      const extractionPrompt = `From the following conversation, extract important information that should be remembered about the user's preferences, needs, or important facts. Return as JSON with key-value pairs.

Question: ${question}
Response: ${response}

Extract information like:
- User preferences
- Important facts mentioned
- User's goals or objectives
- Technical requirements
- Personal information shared
- Context for future interactions

Return only the JSON object, no additional text:`;

      const extraction = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [
          { role: 'user', content: extractionPrompt }
        ],
        temperature: 0.1,
        max_tokens: 500
      });

      try {
        const memories = JSON.parse(extraction.choices[0].message.content);
        
        for (const [key, value] of Object.entries(memories)) {
          await advancedMemoryService.storeMemory(userId, key, value, {
            category: 'extracted_preference',
            importance: 2,
            source: 'conversation_extraction'
          });
        }
      } catch (parseError) {
        logger.warn('Failed to parse extracted memories:', parseError.message);
      }

    } catch (error) {
      logger.warn('Failed to extract memories:', error.message);
    }
  }

  async summarizeContent(content, userId = 'default', options = {}) {
    try {
      const {
        maxLength = 500,
        style = 'comprehensive', // 'brief', 'comprehensive', 'bullet-points'
        focus = 'key-points' // 'key-points', 'technical', 'business', 'academic'
      } = options;

      const stylePrompts = {
        brief: 'Provide a brief, concise summary',
        comprehensive: 'Provide a comprehensive summary that covers all important points',
        'bullet-points': 'Provide a summary in bullet-point format'
      };

      const focusPrompts = {
        'key-points': 'focusing on the main ideas and key takeaways',
        technical: 'focusing on technical details and specifications',
        business: 'focusing on business implications and strategic value',
        academic: 'focusing on research findings and scholarly insights'
      };

      const prompt = `${stylePrompts[style]} of the following content, ${focusPrompts[focus]}. Keep the summary under ${maxLength} words.

Content to summarize:
${content}

Summary:`;

      const response = await this.openai.chat.completions.create({
        model: this.models.chat,
        messages: [
          { role: 'user', content: prompt }
        ],
        temperature: 0.2,
        max_tokens: Math.min(maxLength * 2, 1000)
      });

      const summary = response.choices[0].message.content;

      // Store summary in memory
      await advancedMemoryService.storeMemory(userId, 'content_summary', {
        originalLength: content.length,
        summary,
        style,
        focus
      }, {
        category: 'summary',
        importance: 2
      });

      return summary;

    } catch (error) {
      logger.error('Failed to summarize content:', error);
      throw error;
    }
  }

  async generateEmbedding(text) {
    try {
      const response = await this.openai.embeddings.create({
        model: this.models.embedding,
        input: text,
        encoding_format: 'float',
      });

      return response.data[0].embedding;
    } catch (error) {
      logger.error('Failed to generate embedding:', error);
      throw error;
    }
  }

  async chatWithHistory(message, userId = 'default', options = {}) {
    try {
      const {
        includeFullHistory = false,
        maxHistoryLength = 10,
        temperature = 0.3
      } = options;

      // Get conversation history
      const history = await advancedMemoryService.getConversationHistory(userId, maxHistoryLength);
      const memory = await advancedMemoryService.getContextualMemory(userId, message, 3);

      // Build conversation messages
      const messages = [
        { role: 'system', content: this.systemPrompt }
      ];

      // Add memory context
      if (memory.summary) {
        messages.push({
          role: 'system',
          content: `User Context: ${memory.summary}`
        });
      }

      // Add conversation history
      if (includeFullHistory && history.length > 0) {
        for (const conv of history.reverse()) {
          messages.push({ role: 'user', content: conv.message });
          messages.push({ role: 'assistant', content: conv.response });
        }
      }

      // Add current message
      messages.push({ role: 'user', content: message });

      const response = await this.openai.chat.completions.create({
        model: this.models.chat,
        messages,
        temperature,
        max_tokens: 2000
      });

      const reply = response.choices[0].message.content;

      // Store conversation
      await advancedMemoryService.storeConversation(userId, message, reply, {
        model: this.models.chat,
        historyIncluded: includeFullHistory,
        messageCount: messages.length
      });

      return reply;

    } catch (error) {
      logger.error('Failed to chat with history:', error);
      throw error;
    }
  }
}

module.exports = new AdvancedOpenAIService();
